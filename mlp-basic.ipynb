{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp-basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPwduQjqpJJVus+mqBQO6V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. MLP 구현**"
      ],
      "metadata": {
        "id": "fJVz3EOU6i8Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXD9txG1pkr2",
        "outputId": "8a5de268-9890-4a9c-d7b2-14058ded874b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=3, out_features=3, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.4148, -0.1421, -0.1588],\n",
            "        [-0.2669, -0.0184,  0.2104],\n",
            "        [ 0.0234, -0.0314, -0.1042]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1916,  0.3572,  0.0405], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 다층 퍼셉트론 이진 분류기 모델을 만들기 위해서는 nn.Linear 객체가 두 개 필요\n",
        "fc1 = nn.Linear(3, 3) # 입력과 은닉을 연결\n",
        "fc2 = nn.Linear(3, 2) # 은닉과 출력을 연결\n",
        "\n",
        "print(fc1)\n",
        "\n",
        "# Deep Learning 모델을 살필 때는 파라미터로 가중치랑 편향 확인\n",
        "# 학습이 안 된 상태 - 초기화(랜덤) 값\n",
        "print(fc1.weight)\n",
        "print(fc1.bias)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 첫 번째 계층에 무작위 입력값 통과 (퍼셉트론 연산 수행)\n",
        "x = torch.rand(3)\n",
        "print(\"Input vector x\")\n",
        "print(x, \"\\n\")\n",
        "\n",
        "# 해당 계층의 변수 이름과 괄호를 통해 통과시키고자 하는 입력값을 전달\n",
        "print(\"1st layer input\")\n",
        "print(fc1(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOvALX-9rEtq",
        "outputId": "126ab71b-dac7-4675-ca6d-d78fe22cc1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vector x\n",
            "tensor([0.8588, 0.9737, 0.0774]) \n",
            "\n",
            "1st layer input\n",
            "tensor([0.0139, 0.1264, 0.0220], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력값의 수가 맞지 않는다면 계층이 처리하지 못해 오류 발생\n",
        "x_error = torch.rand(4)\n",
        "print(\"Wrong input vector\")\n",
        "print(x_error, \"\\n\")\n",
        "\n",
        "# 입력 벡터가 fc1의 가중치 벡터와 선형 연산(행렬곱)이 불가능한 형상이기 때문\n",
        "print(\"Error on 1st layer\")\n",
        "print(fc1(x_error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "TZjs5yrOrbAR",
        "outputId": "52ea8bf3-85b0-4079-a9bc-b7915a9c191b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong input vector\n",
            "tensor([0.6698, 0.1871, 0.0650, 0.2628]) \n",
            "\n",
            "Error on 1st layer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ed002215dce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error on 1st layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 3x3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 미니배치 단위\n",
        "\n",
        "# 미니배치 단위가 되면 입력 벡터가 입력 행렬이 되면서 차원이 2로 늘어남\n",
        "# 중요한 건 in_feature=3 (4는 미니배치)\n",
        "x_batch = torch.rand(4, 3)\n",
        "\n",
        "# 행렬곱 -> 앞 행렬의 두 번째 차원, 뒤 행렬의 첫 번째 차원 값만 같으면 수행 가능\n",
        "print(\"fc1 layer weight shape: {}\".format(fc1.weight.shape))\n",
        "print(\"fc1 layer bias shape: {}\".format(fc1.bias.shape))\n",
        "print(\"Input batch output shape: {}\".format(fc1(x_batch).shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWTI9TLvyMnl",
        "outputId": "3ca22c4a-2aa3-432f-e7c7-1cc5a42b98b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1 layer weight shape: torch.Size([3, 3])\n",
            "fc1 layer bias shape: torch.Size([3])\n",
            "Input batch output shape: torch.Size([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  # 첫번째 함수를 통해서 초기화\n",
        "  def __init__(self):\n",
        "    super().__init__() # (nn.Module)에 있는 것들을 다 활용. 초기화 단계. 관례적으로 같이 씀\n",
        "    self.fc1 = nn.Linear(3, 3)\n",
        "    self.fc2 = nn.Linear(3, 2)\n",
        "\n",
        "  # forward 라는 함수명으로 층을 쌓음 (layer 간의 연결)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = torch.sigmoid(x)  # 계층 사이에 활성화 함수\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = F.softmax(x, dim=-1) # 두 번째 계층 이후 소프트맥스 함수 적용\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "lL_i7lEWzKuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def forward : 층을 거쳐가면서 업데이트되는 식으로 처리   \n",
        "입력변수 'x'  \n",
        "- fc1을 거친 결과를 다시 x로 업데이트\n",
        "- 비선형적 활섬화 함수(sigmoid)를 통해 업데이트  \n",
        "- 은닉층에서 출력층으로 가는 fc2를 통해 업데이트  \n",
        "- softmax를 통해 결과값을 확률로 나타냄"
      ],
      "metadata": {
        "id": "11969_X_5CPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP() # 객체 정의 안 해줬을 때 에러\n",
        "\n",
        "x_batch = torch.rand(4, 3)\n",
        "print(\"Model output\")\n",
        "print(model(x_batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-tKfIPk27bj",
        "outputId": "80f529e5-7122-42ee-d4c3-b1881c09444b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output\n",
            "tensor([[0.6888, 0.3112],\n",
            "        [0.6896, 0.3104],\n",
            "        [0.6896, 0.3104],\n",
            "        [0.6903, 0.3097]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 비용함수**"
      ],
      "metadata": {
        "id": "H9MSeBd_6dMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(x)) # 모델이 추정한 각 레이블에 대한 확률분포"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24TTxeV26Ed0",
        "outputId": "25c1f28b-04f7-4d0c-f0ae-ab14dd28f72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6882, 0.3118], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_output = model(x).unsqueeze(0) # 배치 단위만 입력으로 받기 때문에 unsqueeze() 메소드로 \"빈 차원\" 하나를 배치 단위 자리에 추가\n",
        "loss = criterion(model_output, torch.tensor([0]))\n",
        "print(loss) # 계산되어 나온 오차 함수 값\n",
        "\n",
        "# 함수 호출 한 번으로 역전파 알고리즘 수행\n",
        "loss.backward()\n",
        "\n",
        "# 모델의 첫 번째 퍼셉트론 계층 편향에 전달된 기울기값\n",
        "model.fc1.bias.grad "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1tM1hF8HlbY",
        "outputId": "5022bc1b-6959-4459-9cc5-c78ce7db878c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5225, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0026, -0.0274,  0.0168])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}
